{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 23894,
     "status": "ok",
     "timestamp": 1751274162787,
     "user": {
      "displayName": "Dawn Escaf",
      "userId": "09633541017186048933"
     },
     "user_tz": -345
    },
    "id": "KHrhmngJDgRm"
   },
   "outputs": [],
   "source": [
    "# âœ… Optimized EfficientNet-B4 Snake Classifier for Google Colab (With Evaluation, Auto-Split, and Prediction)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1751274162894,
     "user": {
      "displayName": "Dawn Escaf",
      "userId": "09633541017186048933"
     },
     "user_tz": -345
    },
    "id": "g4qmnKz0EJs-",
    "outputId": "efd5d64d-d3c3-4ef4-ff68-475a26c4c937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68987,
     "status": "ok",
     "timestamp": 1751274231890,
     "user": {
      "displayName": "Dawn Escaf",
      "userId": "09633541017186048933"
     },
     "user_tz": -345
    },
    "id": "kRffAD1GEgbu",
    "outputId": "cefd4ca1-6131-49ff-8292-e65501814a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# To Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OcvKrShEGCH",
    "outputId": "bcb0c8b3-489b-41a7-9ce1-cad14e8f97c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ› ï¸ Splitting dataset...\n"
     ]
    }
   ],
   "source": [
    "# 2. Transforms & Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((380, 380)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_path = '/content/drive/MyDrive/Snake_Dataset'\n",
    "split_path = '/content/drive/MyDrive/Split_Snake_Dataset'\n",
    "\n",
    "assert os.path.exists(data_path), f\"Dataset not found at {data_path}\"\n",
    "\n",
    "# Auto Split if not already split\n",
    "if not os.path.exists(split_path):\n",
    "    print(\"ðŸ› ï¸ Splitting dataset...\")\n",
    "    os.makedirs(os.path.join(split_path, 'train'))\n",
    "    os.makedirs(os.path.join(split_path, 'val'))\n",
    "\n",
    "    full_dataset = ImageFolder(data_path)\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    class_names = full_dataset.classes\n",
    "\n",
    "    class_indices = {cls: [] for cls in class_names}\n",
    "    for idx, (img_path, label) in enumerate(full_dataset.samples):\n",
    "        cls = class_names[label]\n",
    "        class_indices[cls].append(img_path)\n",
    "\n",
    "    for cls in class_names:\n",
    "        cls_dir_train = os.path.join(split_path, 'train', cls)\n",
    "        cls_dir_val = os.path.join(split_path, 'val', cls)\n",
    "        os.makedirs(cls_dir_train, exist_ok=True)\n",
    "        os.makedirs(cls_dir_val, exist_ok=True)\n",
    "\n",
    "        imgs = class_indices[cls]\n",
    "        split_idx = int(len(imgs) * 0.85)\n",
    "        for img in imgs[:split_idx]:\n",
    "            shutil.copy(img, os.path.join(cls_dir_train, os.path.basename(img)))\n",
    "        for img in imgs[split_idx:]:\n",
    "            shutil.copy(img, os.path.join(cls_dir_val, os.path.basename(img)))\n",
    "else:\n",
    "    print(\"ðŸ“ Dataset already split.\")\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(split_path, 'train'), transform=transform)\n",
    "val_dataset = ImageFolder(os.path.join(split_path, 'val'), transform=transform)\n",
    "num_classes = len(train_dataset.classes)\n",
    "dataset_classes = train_dataset.classes\n",
    "\n",
    "print(f\"ðŸª² Classes: {num_classes} => {dataset_classes}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSlljZT4D4XF"
   },
   "outputs": [],
   "source": [
    "# 3. Model\n",
    "model = timm.create_model('tf_efficientnet_b4_ns', pretrained=True, drop_rate=0.4, drop_path_rate=0.3)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.BatchNorm1d(model.num_features),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(model.num_features, num_classes)\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"ðŸ¤– Model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3hEhWhWDzQH"
   },
   "outputs": [],
   "source": [
    "# 4. Loss, Optimizer, Scheduler\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WF5v0rVGDnwn"
   },
   "outputs": [],
   "source": [
    "# 5. Train & Validate\n",
    "num_epochs = 15\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        train_bar.set_postfix(loss=running_loss/(total//4), acc=100.*correct/total)\n",
    "\n",
    "    train_acc = 100.*correct/total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = 100.*val_correct/val_total\n",
    "    scheduler.step(epoch + val_acc)\n",
    "    print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_b4_model.pth\")\n",
    "        print(\"âœ… Saved Best Model\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Evaluation Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=dataset_classes))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset_classes, yticklabels=dataset_classes)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKMr0EDqDl2V"
   },
   "outputs": [],
   "source": [
    "# 6. Inference on User Image (for VS Code or local desktop use)\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def predict_image():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select an image\", filetypes=[(\"Image Files\", \"*.jpg *.jpeg *.png\")]\n",
    "    )\n",
    "\n",
    "    if file_path:\n",
    "        img = Image.open(file_path).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                pred = model(img_tensor)\n",
    "            pred_class = pred.argmax(dim=1).item()\n",
    "        print(f\"\\nðŸ§  Prediction for '{os.path.basename(file_path)}': {dataset_classes[pred_class]}\")\n",
    "    else:\n",
    "        print(\"No image selected.\")\n",
    "\n",
    "predict_image()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOBBzbC1Vb1BbRRHsuswKef",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
